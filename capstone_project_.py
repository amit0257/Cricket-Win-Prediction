# -*- coding: utf-8 -*-
"""Capstone project .ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/186dFnfTmD4PU-FM3iDsvReTE8J3nyb4k

from google.colab import drive
"""

from google.colab import drive



drive.mount('/content/drive')

import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt

#importing the data
df=pd.read_excel('/content/drive/My Drive/DLCP/cricket_data.xlsx')

"""Lets check the data"""

df.head()

df.tail()

# shape of the data
df.shape

# lets check the data info
df.info()

#lets divide the data into categorical and numerical
cat=[]
num=[]
for i in df.columns:
    if df[i].dtype=="object":
        cat.append(i)
    else:
        num.append(i)
print(cat)
print(num)

df[cat].describe().T

"""Inference :
### India won 84% of the matches they played
### 70% of the matches India Played are in Day Light
### 63% of the Matches India paid are ODI's
### 59 % of the time atleast 3 people scored zero in match
"""

df[num].describe().T

for column in df.columns:
    if df[column].dtype == 'object':
        print(column.upper(),': ',df[column].nunique())
        print(df[column].value_counts().sort_values())
        print('\n')

#lets understand pecentage wins over oppenents
grouped= df['Opponent'].groupby(df['Result']).value_counts()

grouped / grouped.groupby(level = 1).sum()*100

## Are there any duplicate records

# Check for duplicate data

dups = df.duplicated()
print('Number of duplicate rows = %d' % (dups.sum()))

df[dups]

df.isnull().sum()

# lets impute missing values in categorical data

for column in ['Match_light_type', 'Match_format', 'First_selection', 'Opponent', 'Season', 'Offshore']:
    df[column].fillna(df[column].mode()[0], inplace=True)

cat1=["Match_light_type", "Match_format", "First_selection", "Opponent", "Season", "Offshore"]

for i in cat1:
    df[i]=df[i].astype("object")

# lets impute missing values in numerical data
num1=['Avg_team_Age', 'Bowlers_in_team', 'Wicket_keeper_in_team', 'All_rounder_in_team', 'Audience_number', 'Max_run_scored_1over', 'Max_wicket_taken_1over', 'Extra_bowls_bowled', 'Min_run_given_1over', 'Min_run_scored_1over', 'Max_run_given_1over', 'extra_bowls_opponent', 'player_highest_run']

for i in num1:
    df[i].fillna(df[i].median(),inplace=True)

df.isnull().sum()

"""### All null values have been appropriately imputed"""

print(num)

"""# Univarite Analysis"""

fig, axes = plt.subplots(nrows=4,ncols=2)
fig.set_size_inches(18,18)
a = sns.distplot(df['Avg_team_Age'] , ax=axes[0][0])
a = sns.boxplot(df['Avg_team_Age'] , orient = "v" , ax=axes[0][1])


a = sns.distplot(df['Bowlers_in_team'] , ax=axes[1][0])
a = sns.boxplot(df['Bowlers_in_team'] , orient = "v" , ax=axes[1][1])



a = sns.distplot(df['Wicket_keeper_in_team'] , ax=axes[2][0])
a = sns.boxplot(df['Wicket_keeper_in_team'] , orient = "v" , ax=axes[2][1])



a = sns.distplot(df['All_rounder_in_team'] , ax=axes[3][0])
a = sns.boxplot(df['All_rounder_in_team'] , orient = "v" , ax=axes[3][1])


plt.show()

fig, axes = plt.subplots(nrows=4,ncols=2)
fig.set_size_inches(18,18)
a = sns.distplot(df['Audience_number'] , ax=axes[0][0])
a = sns.boxplot(df['Audience_number'] , orient = "v" , ax=axes[0][1])


a = sns.distplot(df['Max_wicket_taken_1over'] , ax=axes[1][0])
a = sns.boxplot(df['Max_wicket_taken_1over'] , orient = "v" , ax=axes[1][1])



a = sns.distplot(df['Extra_bowls_bowled'] , ax=axes[2][0])
a = sns.boxplot(df['Extra_bowls_bowled'] , orient = "v" , ax=axes[2][1])



a = sns.distplot(df['Max_wicket_taken_1over'] , ax=axes[3][0])
a = sns.boxplot(df['Max_wicket_taken_1over'] , orient = "v" , ax=axes[3][1])


plt.show()

fig, axes = plt.subplots(nrows=5,ncols=2)
fig.set_size_inches(18,18)
a = sns.distplot(df['Min_run_given_1over'] , ax=axes[0][0])
a = sns.boxplot(df['Min_run_given_1over'] , orient = "v" , ax=axes[0][1])


a = sns.distplot(df['Min_run_scored_1over'] , ax=axes[1][0])
a = sns.boxplot(df['Min_run_scored_1over'] , orient = "v" , ax=axes[1][1])



a = sns.distplot(df['Max_run_given_1over'] , ax=axes[2][0])
a = sns.boxplot(df['Max_run_given_1over'] , orient = "v" , ax=axes[2][1])



a = sns.distplot(df['extra_bowls_opponent'] , ax=axes[3][0])
a = sns.boxplot(df['extra_bowls_opponent'] , orient = "v" , ax=axes[3][1])


a = sns.distplot(df['player_highest_run'] , ax=axes[4][0])
a = sns.boxplot(df['player_highest_run'] , orient = "v" , ax=axes[4][1])

plt.show()

"""# Bivariate Analysis"""

print(cat)

sns.countplot(x="Match_light_type", hue="Result", data=df)

sns.countplot(x="Match_format", hue="Result", data=df)

sns.countplot(x="First_selection", hue="Result", data=df)

sns.set(rc={"figure.figsize":(10, 10)})

sns.countplot(x="Opponent", hue="Result", data=df)

sns.countplot(x="Players_scored_zero", hue="Opponent", data=df)

sns.countplot(x="player_highest_wicket", hue="Result", data=df)

"""Line plot"""

sns.countplot(x="Match_format", hue="Opponent", data=df)

df.Players_scored_zero[df.Players_scored_zero == 'Three'] = 3

df.player_highest_wicket[df.player_highest_wicket == 'Three'] = 3

df.replace(to_replace ="20-20",
                 value ="T20")

df=df.replace(to_replace ="Bat",
                 value ="Batting")

#sns.pairplot(df ,diag_kind='kde' ,hue='Result');

plt.figure(figsize=(18,18))
sns.heatmap(df.corr(),annot=True)
plt.show()

# construct box plot for continuous variables
plt.figure(figsize=(10,10))
df.boxplot(vert=0)
plt.show()

#treating outliers
def remove_outlier(num):
    sorted(num)
    Q1,Q3=np.percentile(num,[25,75])
    IQR=Q3-Q1
    lower_range= Q1-(1.5 * IQR)
    upper_range= Q3+(1.5 * IQR)
    return lower_range, upper_range

for column in df[num].columns:
    lr,ur=remove_outlier(df[column])
    df[column]=np.where(df[column]>ur,ur,df[column])
    df[column]=np.where(df[column]<lr,lr,df[column])

plt.figure(figsize=(10,10))
df.boxplot(vert=0)
plt.show()

df.Result[df.Result == 'Win'] = 0
df.Result[df.Result == 'Loss'] = 1

df=df.drop('Game_number',axis=1)

cat2 = ['Match_light_type', 'Match_format', 'First_selection', 'Opponent', 'Season', 'Offshore', 'Players_scored_zero', 'player_highest_wicket']

df1 =pd.get_dummies(df, columns=cat2,drop_first=True)

df1.head()

# importing train_test_split
from sklearn.model_selection import train_test_split

X=df1.drop('Result',axis=1)
y=df1['Result']

y=y.astype('int')

# split into 70:30 ration
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 0)

# describes info about train and test set
print("Number transactions X_train dataset: ", X_train.shape)
print("Number transactions y_train dataset: ", y_train.shape)
print("Number transactions X_test dataset: ", X_test.shape)
print("Number transactions y_test dataset: ", y_test.shape)

from collections import Counter
from sklearn.datasets import make_classification
from imblearn.over_sampling import SMOTE

counter=Counter(y)
counter

from imblearn.over_sampling import SMOTE

oversample = SMOTE()
X, y = oversample.fit_resample(X, y)

counter=Counter(y)
counter

"""# Lets apply Machine Learning modules to see the preidction

"""

from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
from sklearn.metrics import confusion_matrix,classification_report,roc_auc_score,roc_curve
from sklearn import metrics

clf = LinearDiscriminantAnalysis()
model1=clf.fit(X_train,y_train)
model1

y_train_predict=model1.predict(X_train)
model_score=model1.score(X_train, y_train)
print(model_score)
print('  ')
print(metrics.confusion_matrix(y_train,y_train_predict))
print('  ')

print(metrics.classification_report(y_train,y_train_predict))

from sklearn.metrics import roc_auc_score,roc_curve

probs_train=model1.predict_proba(X_train)
probs_train=probs_train[:, 1]
auc=roc_auc_score(y_train,probs_train)
print("the auc %.3f " % auc)
train_fpr ,train_tpr, train_thresholds= roc_curve(y_train,probs_train)
plt.plot([0,1],[0,1],linestyle='--')
plt.plot(train_fpr,train_tpr);

importance = model1.coef_[0]
# summarize feature importance
for i,v in enumerate(importance):
	print('Feature: %0d, Score: %.5f' % (i,v))
# plot feature importance
sns.barplot([x for x in range(len(importance))], importance)
plt.show()

X.info()

y_test_predict=model1.predict(X_test)
model_score=model1.score(X_test, y_test)
print(model_score)
print(metrics.confusion_matrix(y_test,y_test_predict))

print(metrics.classification_report(y_test,y_test_predict))

probs_test=model1.predict_proba(X_test)
probs_test=probs_test[:,1]
auc=roc_auc_score(y_test,probs_test)
print("the auc curve %.3f " % auc)

test_fpr,test_tpr,test_threshold=roc_curve(y_test,probs_test)
plt.plot([0,1],[0,1],linestyle='--')
plt.plot(test_fpr, test_tpr)

"""Naive Bayes"""

from sklearn.naive_bayes import GaussianNB

NB=GaussianNB()
model2=NB.fit(X_train, y_train)

y_train_predict=model2.predict(X_train)
model_score=model2.score(X_train, y_train)
print(model_score)
print('  ')
print(metrics.confusion_matrix(y_train,y_train_predict))
print('  ')

print(metrics.classification_report(y_train,y_train_predict))

probs_train=model2.predict_proba(X_train)
probs_train=probs_train[:, 1]
auc=roc_auc_score(y_train,probs_train)
print("the auc %.3f " % auc)
train_fpr ,train_tpr, train_thresholds= roc_curve(y_train,probs_train)
plt.plot([0,1],[0,1],linestyle='--')
plt.plot(train_fpr,train_tpr);

y_test_predict=model2.predict(X_test)
model_score=model2.score(X_test, y_test)
print(model_score)
print(metrics.confusion_matrix(y_test,y_test_predict))

print(metrics.classification_report(y_test,y_test_predict))

probs_test=model2.predict_proba(X_test)
probs_test=probs_test[:,1]
auc=roc_auc_score(y_test,probs_test)
print("the auc curve %.3f " % auc)

test_fpr,test_tpr,test_threshold=roc_curve(y_test,probs_test)
plt.plot([0,1],[0,1],linestyle='--')
plt.plot(test_fpr, test_tpr)

"""KNN"""

from sklearn.neighbors import KNeighborsClassifier

KNN_model=KNeighborsClassifier()
model3=KNN_model.fit(X_train,y_train)

y_train_predict=model3.predict(X_train)
model_score=model3.score(X_train, y_train)
print(model_score)
print('  ')
print(metrics.confusion_matrix(y_train,y_train_predict))
print('  ')

print(metrics.classification_report(y_train,y_train_predict))

probs_train=model3.predict_proba(X_train)
probs_train=probs_train[:, 1]
auc=roc_auc_score(y_train,probs_train)
print("the auc %.3f " % auc)
train_fpr ,train_tpr, train_thresholds= roc_curve(y_train,probs_train)
plt.plot([0,1],[0,1],linestyle='--')
plt.plot(train_fpr,train_tpr);

y_test_predict=model3.predict(X_test)
model_score=model3.score(X_test, y_test)
print(model_score)
print(metrics.confusion_matrix(y_test,y_test_predict))

print(metrics.classification_report(y_test,y_test_predict))

probs_test=model3.predict_proba(X_test)
probs_test=probs_test[:,1]
auc=roc_auc_score(y_test,probs_test)
print("the auc curve %.3f " % auc)

test_fpr,test_tpr,test_threshold=roc_curve(y_test,probs_test)
plt.plot([0,1],[0,1],linestyle='--')
plt.plot(test_fpr, test_tpr)

"""# Decision Tree

"""

from sklearn import tree

DT_model=tree.DecisionTreeClassifier(max_depth=8, random_state=42)
model4=DT_model.fit(X_train,y_train)

y_train_predict=model4.predict(X_train)
model_score=model4.score(X_train, y_train)
print(model_score)
print('  ')
print(metrics.confusion_matrix(y_train,y_train_predict))
print('  ')

print(metrics.classification_report(y_train,y_train_predict))

probs_train=model4.predict_proba(X_train)
probs_train=probs_train[:, 1]
auc=roc_auc_score(y_train,probs_train)
print("the auc %.3f " % auc)
train_fpr ,train_tpr, train_thresholds= roc_curve(y_train,probs_train)
plt.plot([0,1],[0,1],linestyle='--')
plt.plot(train_fpr,train_tpr);

y_test_predict=model4.predict(X_test)
model_score=model4.score(X_test, y_test)
print(model_score)
print(metrics.confusion_matrix(y_test,y_test_predict))

print(metrics.classification_report(y_test,y_test_predict))

probs_test=model4.predict_proba(X_test)
probs_test=probs_test[:,1]
auc=roc_auc_score(y_test,probs_test)
print("the auc curve %.3f " % auc)

test_fpr,test_tpr,test_threshold=roc_curve(y_test,probs_test)
plt.plot([0,1],[0,1],linestyle='--')
plt.plot(test_fpr, test_tpr)

importance = model4.feature_importances_
# summarize feature importance
for i,v in enumerate(importance):
	print('Feature: %0d, Score: %.5f' % (i,v))
# plot feature importance
sns.barplot([x for x in range(len(importance))], importance)
plt.show()

"""# Ensemble Methods

## Random Forrest
"""

from sklearn.ensemble import RandomForestClassifier

rf_model=RandomForestClassifier()
model5=rf_model.fit(X_train,y_train)

y_train_predict=model5.predict(X_train)
model_score=model5.score(X_train, y_train)
print(model_score)
print('  ')
print(metrics.confusion_matrix(y_train,y_train_predict))
print('  ')

print(metrics.classification_report(y_train,y_train_predict))

importance = model5.feature_importances_
# summarize feature importance
for i,v in enumerate(importance):
	print('Feature: %0d, Score: %.5f' % (i,v))
# plot feature importance
sns.barplot([x for x in range(len(importance))], importance)
plt.show()

y_test_predict=model5.predict(X_test)
model_score=model5.score(X_test, y_test)
print(model_score)
print(metrics.confusion_matrix(y_test,y_test_predict))

print(metrics.classification_report(y_test,y_test_predict))

probs_train=model5.predict_proba(X_train)
probs_train=probs_train[:, 1]
auc=roc_auc_score(y_train,probs_train)
print("the auc for train %.3f " % auc)

probs_test=model5.predict_proba(X_test)
probs_test=probs_test[:,1]
auc=roc_auc_score(y_test,probs_test)
print("the auc for test %.3f " % auc)

from sklearn.model_selection import GridSearchCV

param_grid = {  'bootstrap': [True], 'max_depth': [5, 10, None], 'max_features': ['auto', 'log2'], 'n_estimators': [5, 6, 7, 8, 9, 10, 11, 12, 13, 15]}

g_search_RF = GridSearchCV(estimator = rf_model, param_grid = param_grid,

                          cv = 3, n_jobs = 1, verbose = 0, return_train_score=True)

rf_tuned=g_search_RF.fit(X_train,y_train)

y_train_predict=rf_tuned.predict(X_train)
model_score=rf_tuned.score(X_train, y_train)
print(model_score)
print('  ')
print(metrics.confusion_matrix(y_train,y_train_predict))
print('  ')

print(metrics.classification_report(y_train,y_train_predict))

y_test_predict=rf_tuned.predict(X_test)
model_score=rf_tuned.score(X_test, y_test)
print(model_score)
print(metrics.confusion_matrix(y_test,y_test_predict))

print(metrics.classification_report(y_test,y_test_predict))

probs_train=rf_tuned.predict_proba(X_train)
probs_train=probs_train[:, 1]
auc=roc_auc_score(y_train,probs_train)
print("the auc %.3f " % auc)

probs_test=rf_tuned.predict_proba(X_test)
probs_test=probs_test[:,1]
auc=roc_auc_score(y_test,probs_test)
print("the auc curve %.3f " % auc)



"""## Bagging Classifier"""

from sklearn.ensemble import BaggingClassifier

bgcl = BaggingClassifier(n_estimators=100,random_state=1)

model6 = bgcl.fit(X_train, y_train)

y_train_predict=model6.predict(X_train)
model_score=model6.score(X_train, y_train)
print(model_score)
print('  ')
print(metrics.confusion_matrix(y_train,y_train_predict))
print('  ')

print(metrics.classification_report(y_train,y_train_predict))

y_test_predict=model6.predict(X_test)
model_score=model6.score(X_test, y_test)
print(model_score)
print(metrics.confusion_matrix(y_test,y_test_predict))

print(metrics.classification_report(y_test,y_test_predict))

probs_train=model6.predict_proba(X_train)
probs_train=probs_train[:, 1]
auc=roc_auc_score(y_train,probs_train)
print("the auc for train %.3f " % auc)

probs_test=model6.predict_proba(X_test)
probs_test=probs_test[:,1]
auc=roc_auc_score(y_test,probs_test)
print("the auc for test %.3f " % auc)

"""Model Tuning for Bagging"""

param_grid = {
    'base_estimator__max_depth' : [1, 2, 3, 4, 5],
    'max_samples' : [0.05, 0.1, 0.2, 0.5]
}

clf = GridSearchCV(BaggingClassifier(RandomForestClassifier(),
                                     n_estimators = 100, max_features = 0.5 ),
                   param_grid)
bc_tned= clf.fit(X_train, y_train)

print(clf.best_params_)

y_train_predict=bc_tned.predict(X_train)
model_score=bc_tned.score(X_train, y_train)
print(model_score)
print('  ')
print(metrics.confusion_matrix(y_train,y_train_predict))
print('  ')

print(metrics.classification_report(y_train,y_train_predict))

y_test_predict=bc_tned.predict(X_test)
model_score=bc_tned.score(X_test, y_test)
print(model_score)
print(metrics.confusion_matrix(y_test,y_test_predict))

print(metrics.classification_report(y_test,y_test_predict))

probs_train=bc_tned.predict_proba(X_train)
probs_train=probs_train[:, 1]
auc=roc_auc_score(y_train,probs_train)
print("the auc for train %.3f " % auc)

probs_test=bc_tned.predict_proba(X_test)
probs_test=probs_test[:,1]
auc=roc_auc_score(y_test,probs_test)
print("the auc for test %.3f " % auc)

"""## ADA BOOSTING"""

from sklearn.ensemble import AdaBoostClassifier

clfADB = AdaBoostClassifier(n_estimators=100)
model7=clfADB.fit(X_train,y_train)

y_train_predict=model7.predict(X_train)
model_score=model7.score(X_train, y_train)
print(model_score)
print('  ')
print(metrics.confusion_matrix(y_train,y_train_predict))
print('  ')

print(metrics.classification_report(y_train,y_train_predict))

importance = model7.feature_importances_
# summarize feature importance
for i,v in enumerate(importance):
	print('Feature: %0d, Score: %.5f' % (i,v))
# plot feature importance
sns.barplot([x for x in range(len(importance))], importance)
plt.show()

y_test_predict=model7.predict(X_test)
model_score=model7.score(X_test, y_test)
print(model_score)
print(metrics.confusion_matrix(y_test,y_test_predict))

print(metrics.classification_report(y_test,y_test_predict))

probs_train=model7.predict_proba(X_train)
probs_train=probs_train[:, 1]
auc=roc_auc_score(y_train,probs_train)
print("the auc for train %.3f " % auc)

probs_test=model7.predict_proba(X_test)
probs_test=probs_test[:,1]
auc=roc_auc_score(y_test,probs_test)
print("the auc for test %.3f " % auc)

"""## Gradient Bossting"""

from sklearn.ensemble import GradientBoostingClassifier
gbcl = GradientBoostingClassifier(n_estimators = 50,random_state=1)
model8 = gbcl.fit(X_train, y_train)

y_train_predict=model8.predict(X_train)
model_score=model8.score(X_train, y_train)
print(model_score)
print('  ')
print(metrics.confusion_matrix(y_train,y_train_predict))
print('  ')

print(metrics.classification_report(y_train,y_train_predict))

importance = model8.feature_importances_
# summarize feature importance
for i,v in enumerate(importance):
	print('Feature: %0d, Score: %.5f' % (i,v))
# plot feature importance
sns.barplot([x for x in range(len(importance))], importance)
plt.show()

y_test_predict=model8.predict(X_test)
model_score=model8.score(X_test, y_test)
print(model_score)
print(metrics.confusion_matrix(y_test,y_test_predict))

print(metrics.classification_report(y_test,y_test_predict))



